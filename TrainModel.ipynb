{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Nescessary Library</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "from keras.api.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocess (Cleanse Data)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv(\"Dataset/Sentiment_Stock_data.csv\", usecols=[\"Sentiment\", \"Sentence\"])\n",
    "# print(df.to_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal With Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null value rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Check Null\n",
    "print(df.isnull().sum())\n",
    "# print(df.to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with Error Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char= r'[a-zA-Z0-9]+(?:\\s+[a-zA-Z0-9]+)*'\n",
    "\n",
    "# Find rows where \"Sentence\" contains special characters\n",
    "special_char_row = df[~df[\"Sentence\"].str.contains(special_char, regex=True)].index\n",
    "# print(len(special_char_row))\n",
    "\n",
    "# Drop special character rows\n",
    "df.drop(special_char_row, axis=0, inplace=True)\n",
    "\n",
    "error_encode = \"+Ã±\"\n",
    "\n",
    "# Find rows where \"Sentence\" contains special characters\n",
    "error_encode_row = df[df[\"Sentence\"].str.contains(error_encode, regex=False)].index\n",
    "# print(len(error_encode_row))\n",
    "\n",
    "# Drop error encoded rows\n",
    "df.drop(error_encode_row, axis=0, inplace=True)\n",
    "\n",
    "# print(df.to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Tokenization & Padding</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter\n",
    "vocab = 30000\n",
    "embed_dim = 100\n",
    "input_length = 20\n",
    "optimizer = Adam(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab)\n",
    "tokenizer.fit_on_texts(df[\"Sentence\"])\n",
    "\n",
    "# Convert sentences to sequences of token ex. [340, 2, 14467, 1, 72, 15, 48, 220, 2, 352, 62, 702, 2, 73, 6316, 14, 9, 281, 1, 72, 9, 683]\n",
    "tokenized = tokenizer.texts_to_sequences(df[\"Sentence\"])\n",
    "\n",
    "# Padding Sentence to same length\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(df[\"Sentence\"]), maxlen=input_length)\n",
    "\n",
    "# Change to numpy array for training\n",
    "y = np.array(df[\"Sentiment\"], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained GloVe Embeddings\n",
    "glove_path = \"glove.6B/glove.6B.100d.txt\"\n",
    "embedding_index = {}\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "# Create Embedding Matrix\n",
    "embedding_matrix = np.zeros((vocab, embed_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab:\n",
    "        vector = embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #************** For view indexing *******\n",
    "# word_index = tokenizer.word_index  # vocabulary\n",
    "# index_word = {v: k for k, v in word_index.items()}  # token to word mapping\n",
    "\n",
    "# print(df[df[\"Sentence\"].str.contains(\"saudi arabia about\", case=False)])\n",
    "# print(X_train[0]) # Numpy array\n",
    "# print(Y_train[0]) # Pandas Series\n",
    "# # Function to decode tokenized sequences back to text\n",
    "# def decode_sequence(sequence: list):\n",
    "#     for pharse in sequence:\n",
    "#         print(pharse)\n",
    "#         print(\" \".join(index_word.get(token, \"<UNK>\") for token in pharse), \"\\n\")\n",
    "\n",
    "# # Decode and compare tokenized sequences\n",
    "# decode_sequence(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X), X.dtype, X.shape)\n",
    "print(type(y), y.dtype, y.shape)\n",
    "# print([X_train[x] for x in range(5)])\n",
    "# print([Y_train[y] for y in range(5)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Spilt data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the entire DataFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(df[df[\"Sentence\"].str.match(X_train[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), X_train.dtype, X_train.shape)\n",
    "print(type(y_train), y_train.dtype, y_train.shape)\n",
    "print(type(X_test), X_test.dtype, X_test.shape)\n",
    "print(type(y_test), y_test.dtype, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Build Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab,embed_dim,input_length=input_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))  #try different values\n",
    "model.add(Dense(1,activation='sigmoid')) #sigmoid since the output is binary\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot Weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.layers[0].get_weights()[0]  # Extracting embedding weights\n",
    "embedding_weights_flat = embedding_weights.flatten()\n",
    "\n",
    "# Create an index for the weights\n",
    "embedding_index = np.arange(len(embedding_weights_flat))\n",
    "\n",
    "# Plot the embedding weights\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(embedding_index, embedding_weights_flat, marker='o', markersize=2, color='green')\n",
    "plt.title('Embedding Weights')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Embedding Weight Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, recurrent_kernel, bias = model.layers[2].get_weights()  # Extract weights from LSTM layer\n",
    "kernel_flat = kernel.flatten()\n",
    "recurrent_kernel_flat = recurrent_kernel.flatten()\n",
    "\n",
    "# Create indices for the weights\n",
    "kernel_index = np.arange(len(kernel_flat))\n",
    "recurrent_kernel_index = np.arange(len(recurrent_kernel_flat))\n",
    "\n",
    "# Plotting the kernel weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot kernel weights (input-to-hidden)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(kernel_index, kernel_flat, marker='o', linestyle='-', markersize=2, color='blue')\n",
    "plt.title('Input-to-Hidden Weights (Kernel)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "# Plot recurrent kernel weights (hidden-to-hidden)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recurrent_kernel_index, recurrent_kernel_flat, marker='o', linestyle='-', markersize=2, color='red')\n",
    "plt.title('Hidden-to-Hidden Weights (Recurrent Kernel)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
